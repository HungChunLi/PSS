{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install NLTK POS Tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:03:44.917515Z",
     "start_time": "2019-06-15T14:03:32.398477Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jirlong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jirlong/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-15T14:03:17.567965Z",
     "start_time": "2019-06-15T14:03:17.545703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN'), ('eats', 'VBZ'), ('the', 'DT'), ('big', 'JJ'), ('hotdog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"The dog eats the big hotdog.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(nltk.pos_tag(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('book', 'NN'), ('is', 'VBZ'), ('written', 'VBN'), ('by', 'IN'), ('my', 'PRP$'), ('father', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(\"The book is written by my father.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('father', 'NN'), ('has', 'VBZ'), ('written', 'VBN'), ('more', 'JJR'), ('than', 'IN'), ('ten', 'JJ'), ('books', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(\"My father has written more than ten books.\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full list of the Penn POS tags\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform POS tagging for all tokens in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 29364\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/corpus01.txt\", encoding=\"utf8\") as fin:\n",
    "    text = fin.read()\n",
    "print(\"Number of characters: %d\" % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens_with_tag = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most frequent nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bourgeoisie', 51), ('class', 35), ('society', 32), ('bourgeois', 25), ('production', 25), ('proletariat', 25), ('industry', 22), ('development', 14), ('labor', 13), ('existence', 13), ('property', 10), ('history', 9), ('struggle', 8), ('place', 8), ('machinery', 8), ('proportion', 8), ('competition', 8), ('exchange', 7), ('capital', 7), ('man', 7)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NN':\n",
    "        noun_counts[word] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can always convert words into lower case, excepting proper nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bourgeoisie', 51), ('class', 35), ('society', 33), ('bourgeois', 25), ('production', 25), ('proletariat', 25), ('industry', 22), ('development', 14), ('labor', 13), ('existence', 13), ('property', 10), ('history', 9), ('struggle', 8), ('place', 8), ('machinery', 8), ('proportion', 8), ('competition', 8), ('exchange', 7), ('capital', 7), ('man', 7)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NN':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore other kinds of part of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 9), ('thus', 3), ('become', 2), ('have', 2), ('form', 2), ('attain', 2), ('fortify', 2), ('slave', 1), ('commerce', 1), ('pass', 1), ('show', 1), ('bring', 1), ('exist', 1), ('distinguish', 1), ('ossify', 1), ('face', 1), ('nestle', 1), ('settle', 1), ('establish', 1), ('work', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'VB':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('America', 4), ('France', 4), ('Modern', 3), ('State', 3), ('England', 3), ('Society', 2), ('East', 2), ('Industry', 2), ('Germany', 2), ('_i.e._', 2), ('Nay', 2), ('BOURGEOIS', 1), ('PROLETARIANS', 1), ('Freeman', 1), ('Rome', 1), ('Bourgeoisie', 1), ('Proletariat', 1), ('Cape', 1), ('Thereupon', 1), ('Italy', 1)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag == 'NNP' or tag == 'NNPS':\n",
    "        noun_counts[word] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 56), ('has', 46), ('are', 32), ('have', 14), ('was', 11), ('be', 9), ('become', 6), ('becomes', 6), ('were', 5), ('developed', 5), ('created', 5), ('existing', 4), ('carried', 4), ('find', 4), ('put', 4), ('set', 4), ('been', 4), ('destroyed', 4), ('being', 4), ('had', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'V':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        \n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With lemmatization for better handle different forms of verbs.\n",
    "\n",
    "Load WordNet Lemmatizer provided by NTLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('be', 121), ('have', 65), ('become', 15), ('find', 9), ('develop', 8), ('do', 6), ('take', 6), ('compel', 6), ('destroy', 6), ('create', 6), ('exist', 5), ('see', 5), ('make', 5), ('go', 5), ('carry', 4), ('rise', 4), ('give', 4), ('increase', 4), ('put', 4), ('set', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'V':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'v')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bourgeoisie', 52), ('class', 49), ('society', 35), ('industry', 28), ('bourgeois', 27), ('proletariat', 26), ('production', 26), ('condition', 21), ('mean', 14), ('development', 14), ('labor', 13), ('relation', 13), ('existence', 13), ('struggle', 10), ('market', 10), ('laborer', 10), ('country', 10), ('property', 10), ('force', 10), ('history', 9)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'N':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'n')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('more', 21), ('not', 20), ('up', 13), ('so', 12), ('only', 10), ('longer', 8), ('therefore', 8), ('no', 7), ('also', 7), ('now', 6), ('everywhere', 6), ('ever', 6), ('most', 6), ('too', 6), ('here', 5), ('then', 5), ('already', 5), ('away', 4), ('just', 4), ('almost', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'R':\n",
    "        noun_counts[word.lower()] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('more', 21), ('not', 20), ('up', 13), ('so', 12), ('only', 10), ('longer', 8), ('therefore', 8), ('no', 7), ('also', 7), ('now', 6), ('everywhere', 6), ('ever', 6), ('most', 6), ('too', 6), ('here', 5), ('far', 5), ('then', 5), ('already', 5), ('away', 4), ('just', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "noun_counts = Counter()\n",
    "for word, tag in tokens_with_tag:\n",
    "    if tag[0] == 'R':\n",
    "        noun_counts[wordnet_lemmatizer.lemmatize(word.lower(), 'r')] += 1\n",
    "        # ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "\n",
    "print(noun_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining Specific Distant Collocations\n",
    "\n",
    "Back to last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\tof\t2\t103\n",
      "of\tthe\t1\t93\n",
      "the\tthe\t3\t64\n",
      ".\tThe\t1\t51\n",
      ",\tthe\t1\t50\n",
      ",\t,\t4\t48\n",
      "the\t,\t2\t48\n",
      "of\t,\t2\t47\n",
      ",\t,\t8\t46\n",
      ",\t,\t5\t46\n",
      ",\t,\t6\t46\n",
      ",\tand\t1\t44\n",
      "the\t,\t5\t43\n",
      ",\tthe\t4\t42\n",
      ",\tthe\t8\t40\n",
      ",\t,\t2\t40\n",
      ",\tthe\t5\t40\n",
      ",\tthe\t6\t39\n",
      "the\tthe\t7\t39\n",
      ",\tthe\t2\t39\n"
     ]
    }
   ],
   "source": [
    "window_size = 9\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "word_pair_distance_counts = Counter()\n",
    "for i in range(len(tokens) - 1):\n",
    "    for distance in range(1, window_size):\n",
    "        if i + distance < len(tokens):\n",
    "            w1 = tokens[i]\n",
    "            w2 = tokens[i + distance]\n",
    "            word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "            word_pair_counts[(w1, w2)] += 1\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%d\" % (w1, w2, distance, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect all verb-noun collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\tsociety\t5\t4\n",
      "be\tclass\t4\t3\n",
      "be\tbourgeoisie\t8\t3\n",
      "be\tclass\t7\t3\n",
      "exist\tsociety\t1\t2\n",
      "be\tstruggle\t5\t2\n",
      "have\tantagonism\t6\t2\n",
      "have\tsociety\t6\t2\n",
      "pave\tway\t2\t2\n",
      "have\tpart\t5\t2\n",
      "get\thand\t3\t2\n",
      "have\tfeudal\t6\t2\n",
      "have\tman\t7\t2\n",
      "be\tman\t6\t2\n",
      "be\tair\t4\t2\n",
      "have\tindustry\t7\t2\n",
      "be\tindustry\t4\t2\n",
      "have\tcountry\t3\t2\n",
      "have\tmean\t5\t2\n",
      "have\tproduction\t7\t2\n"
     ]
    }
   ],
   "source": [
    "window_size = 9\n",
    "\n",
    "word_pair_counts = Counter()\n",
    "word_pair_distance_counts = Counter()\n",
    "for i in range(len(tokens_with_tag) - 1):\n",
    "    w1, t1 = tokens_with_tag[i]\n",
    "    if t1[0] != 'V':\n",
    "        continue\n",
    "    w1 = wordnet_lemmatizer.lemmatize(w1.lower(), 'v')\n",
    "        \n",
    "    for distance in range(1, window_size):\n",
    "        if i + distance < len(tokens_with_tag):\n",
    "            w2, t2 = tokens_with_tag[i + distance]\n",
    "            if t2[0] == 'N':\n",
    "                w2 = wordnet_lemmatizer.lemmatize(w2.lower(), 'n')\n",
    "                word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                word_pair_counts[(w1, w2)] += 1\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common(20):\n",
    "    print(\"%s\\t%s\\t%d\\t%d\" % (w1, w2, distance, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the mean distance of each verb-noun pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_mean_distances = Counter()\n",
    "\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the longest, middle, and shortest pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\tforce\t7.500000\t2\n",
      "have\tmaster\t7.500000\t2\n",
      "have\tman\t7.000000\t2\n",
      "have\tindustry\t7.000000\t2\n",
      "be\tmodern\t7.000000\t2\n",
      "leave\tman\t7.000000\t2\n",
      "be\tcapitalist\t7.000000\t2\n",
      "have\tcondition\t6.500000\t2\n",
      "give\tnavigation\t6.500000\t2\n",
      "push\tclass\t6.500000\t2\n",
      "have\tmarket\t6.500000\t2\n",
      "be\tmean\t6.500000\t2\n",
      "be\tproportion\t6.500000\t2\n",
      "seek\tstatus\t6.500000\t2\n",
      "be\ttime\t6.500000\t2\n",
      "have\tsociety\t6.333333\t3\n",
      "be\tcompetition\t6.200000\t5\n",
      "be\tbourgeoisie\t6.142857\t7\n",
      "have\tfeudal\t6.000000\t2\n",
      "be\texistence\t6.000000\t3\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), distance in pair_mean_distances.most_common(20):\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\texistence\t6.000000\t3\n",
      "be\tcrisis\t6.000000\t2\n",
      "remain\tman\t6.000000\t2\n",
      "work\tmaterial\t6.000000\t2\n",
      "develop\tlaborer\t6.000000\t2\n",
      "have\tcharacter\t6.000000\t2\n",
      "do\tenemy\t6.000000\t2\n",
      "be\tview\t6.000000\t2\n",
      "be\tform\t6.000000\t3\n",
      "have\tproduction\t6.000000\t3\n",
      "be\tindustry\t5.800000\t5\n",
      "be\tclass\t5.545455\t11\n",
      "exist\thistory\t5.500000\t2\n",
      "reduce\trelation\t5.500000\t2\n",
      "have\tantagonism\t5.333333\t3\n",
      "be\tman\t5.333333\t3\n",
      "be\tcondition\t5.333333\t3\n",
      "be\tproletariat\t5.333333\t3\n",
      "be\tstruggle\t5.000000\t4\n",
      "have\tpart\t5.000000\t2\n",
      "be\tbourgeois\t5.000000\t2\n",
      "be\tlaborer\t5.000000\t2\n",
      "compel\tproletariat\t5.000000\t2\n",
      "be\tguild\t5.000000\t2\n",
      "be\tadvance\t5.000000\t2\n",
      "have\tbourgeoisie\t5.000000\t2\n",
      "have\thand\t5.000000\t2\n",
      "have\tpopulation\t5.000000\t3\n",
      "have\tdependent\t5.000000\t2\n",
      "be\tslave\t5.000000\t2\n",
      "be\tsociety\t4.800000\t5\n",
      "have\tmean\t4.750000\t4\n",
      "have\tworld\t4.500000\t2\n",
      "create\tforce\t4.500000\t2\n",
      "join\tclass\t4.500000\t2\n",
      "be\tair\t4.000000\t2\n",
      "be\tweapon\t4.000000\t2\n",
      "have\tclass\t4.000000\t3\n",
      "be\tproduct\t4.000000\t2\n",
      "have\tfamily\t4.000000\t2\n"
     ]
    }
   ],
   "source": [
    "num_pairs = len(pair_mean_distances)\n",
    "mid = num_pairs // 2\n",
    "for (w1, w2), distance in pair_mean_distances.most_common()[mid-20:mid+20]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\tfamily\t4.000000\t2\n",
      "make\tdependent\t4.000000\t2\n",
      "concentrate\thand\t4.000000\t2\n",
      "fight\tenemy\t4.000000\t2\n",
      "have\tcountry\t3.666667\t3\n",
      "work\tclass\t3.500000\t2\n",
      "be\tmovement\t3.333333\t3\n",
      "get\thand\t3.000000\t2\n",
      "be\thistory\t3.000000\t3\n",
      "revolutionize\tproduction\t3.000000\t2\n",
      "make\tcountry\t3.000000\t2\n",
      "be\tlabor\t3.000000\t3\n",
      "find\twork\t2.666667\t3\n",
      "fight\tbourgeoisie\t2.500000\t2\n",
      "oppress\tclass\t2.500000\t2\n",
      "pave\tway\t2.000000\t2\n",
      "supply\tproletariat\t2.000000\t2\n",
      "stand\tface\t2.000000\t2\n",
      "exist\tsociety\t1.000000\t2\n",
      "rule\tclass\t1.000000\t2\n"
     ]
    }
   ],
   "source": [
    "for (w1, w2), distance in pair_mean_distances.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%d\" % (w1, w2, distance, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out the meaningful verb/noun pairs with deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compel\tproletariat\t5.000000\t0.000000\t2\n",
      "see\tproletariat\t7.000000\t0.000000\t2\n",
      "supply\tproletariat\t2.000000\t0.000000\t2\n",
      "have\tdissolution\t7.000000\t0.000000\t2\n",
      "be\tname\t2.000000\t0.000000\t2\n",
      "be\tantagonism\t4.000000\t0.000000\t2\n",
      "base\tantagonism\t3.000000\t0.000000\t2\n",
      "transform\tproperty\t3.000000\t0.000000\t2\n",
      "dominate\tsociety\t6.000000\t0.000000\t2\n",
      "have\tmeaning\t2.000000\t0.000000\t2\n",
      "determine\tcondition\t4.000000\t0.000000\t2\n",
      "admit\tcase\t3.000000\t0.000000\t2\n",
      "be\tcase\t8.000000\t0.000000\t2\n",
      "introduce\tcommunity\t1.000000\t0.000000\t2\n",
      "introduce\twoman\t3.000000\t0.000000\t2\n",
      "introduce\tbourgeoisie\t8.000000\t0.000000\t2\n",
      "keep\tpace\t2.000000\t0.000000\t2\n",
      "organize\tclass\t4.000000\t0.000000\t2\n",
      "lose\tsight\t1.000000\t0.000000\t2\n",
      "be\taction\t7.000000\t0.000000\t2\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n",
      "rise\tbourgeoisie\t1.000000\t0.000000\t2\n",
      "get\thand\t3.000000\t0.000000\t2\n",
      "bourgeois\tsociety\t1.000000\t0.000000\t2\n",
      "compel\tproletariat\t5.000000\t0.000000\t2\n",
      "see\tproletariat\t7.000000\t0.000000\t2\n",
      "supply\tproletariat\t2.000000\t0.000000\t2\n",
      "base\tantagonism\t3.000000\t0.000000\t2\n",
      "transform\tproperty\t3.000000\t0.000000\t2\n",
      "dominate\tsociety\t6.000000\t0.000000\t2\n",
      "determine\tcondition\t4.000000\t0.000000\t2\n",
      "admit\tcase\t3.000000\t0.000000\t2\n",
      "introduce\tcommunity\t1.000000\t0.000000\t2\n",
      "introduce\twoman\t3.000000\t0.000000\t2\n",
      "introduce\tbourgeoisie\t8.000000\t0.000000\t2\n",
      "keep\tpace\t2.000000\t0.000000\t2\n",
      "organize\tclass\t4.000000\t0.000000\t2\n",
      "lose\tsight\t1.000000\t0.000000\t2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if w1 in stopword_list:\n",
    "        continue\n",
    "    if word_pair_counts[(w1, w2)] > 1:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further filter out the low frequent pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "pair_deviations = Counter()\n",
    "for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "    if w1 in stopword_list:\n",
    "        continue\n",
    "    if word_pair_counts[(w1, w2)] > 2:\n",
    "        pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "    s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "    pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "for (w1, w2), dev in pair_deviations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General method for distant collocation mining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A handy lemmatizer \n",
    "# WordNet Style: ADJ (a), ADJ_SAT (s), ADV (r), NOUN (n) or VERB (v)\n",
    "# Penn Style: ADJ (J*), ADJ_SAT (J*), ADV (R*), NOUN (N*), or VERB (V*) \n",
    "def lemmatize_verbose(word, pos):\n",
    "    if pos[0] == 'J':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'a')\n",
    "    elif pos[0] == 'R':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'r')\n",
    "    elif pos[0] == 'N':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'n')\n",
    "    elif pos[0] == 'V':\n",
    "        return wordnet_lemmatizer.lemmatize(word, 'v')\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "\n",
    "def lemmatize_shorter(word, pos):\n",
    "    if pos[0] == 'J':\n",
    "        pos = 'a'\n",
    "    elif pos[0] == 'R':\n",
    "        pos = 'r'\n",
    "    elif pos[0] == 'N':\n",
    "        pos = 'n'\n",
    "    elif pos[0] == 'V':\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        return word\n",
    "    return wordnet_lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "\n",
    "def lemmatize_smarter(word, pos):\n",
    "    if pos[0] in ['R', 'N', 'V']:\n",
    "        pos = pos[0].lower()\n",
    "    elif pos[0] == 'J':\n",
    "        pos = 'a'\n",
    "    else:\n",
    "        return word\n",
    "    return wordnet_lemmatizer.lemmatize(word, pos)\n",
    "\n",
    "\n",
    "# Recommended implementation.\n",
    "def lemmatize(word, pos):\n",
    "    mapping = {'J': 'a', 'R': 'r', 'N': 'n', 'V': 'v'}\n",
    "    if pos[0] in mapping:\n",
    "        return wordnet_lemmatizer.lemmatize(word, mapping[pos[0]])\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count all pairs.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distant_collocations(tokens_with_tag, pos1, pos2, min_cut=2, window_size=9):\n",
    "    word_pair_counts = Counter()\n",
    "    word_pair_distance_counts = Counter()\n",
    "    for i in range(len(tokens_with_tag) - 1):\n",
    "        w1, t1 = tokens_with_tag[i]\n",
    "        if not t1.startswith(pos1):\n",
    "            continue\n",
    "        w1 = lemmatize(w1.lower(), t1)\n",
    "        for distance in range(1, window_size):\n",
    "            if i + distance < len(tokens_with_tag):\n",
    "                w2, t2 = tokens_with_tag[i + distance]\n",
    "                if t2.startswith(pos2):\n",
    "                    w2 = lemmatize(w2.lower(), t2)\n",
    "                    word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                    word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "    pair_mean_distances = Counter()\n",
    "\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if word_pair_counts[(w1, w2)] > 1:\n",
    "            pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "    pair_deviations = Counter()\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if w1 in stopword_list:\n",
    "            continue\n",
    "        if word_pair_counts[(w1, w2)] > min_cut:\n",
    "            pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "    for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "        s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "        pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "    return pair_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'V', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode\tproduction\t0.000000\t0.000000\t0\n",
      "socialist\tcommunist\t0.000000\t0.000000\t0\n",
      "division\tlabor\t0.000000\t0.000000\t0\n",
      "community\twoman\t0.000000\t0.000000\t0\n",
      "world\tmarket\t0.000000\t0.000000\t0\n",
      "condition\tlife\t0.000000\t0.000000\t0\n",
      "mean\tsubsistence\t0.000000\t0.000000\t0\n",
      "form\tsociety\t0.000000\t0.000000\t0\n",
      "form\tproperty\t0.000000\t0.000000\t0\n",
      "member\tsociety\t0.000000\t0.000000\t0\n",
      "state\tthing\t0.000000\t0.000000\t0\n",
      "disappearance\tclass\t0.000000\t0.000000\t0\n",
      "benefit\tclass\t0.000000\t0.000000\t0\n",
      "relation\tproduction\t0.000000\t0.000000\t0\n",
      "mean\tcommunication\t0.000000\t0.000000\t0\n",
      "portion\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "section\tclass\t0.000000\t0.000000\t0\n",
      "state\tsociety\t0.000000\t0.000000\t0\n",
      "hand\tstate\t0.000000\t0.000000\t0\n",
      "bourgeois\tsocialism\t0.000000\t0.000000\t1\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'N', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productive\tforce\t0.000000\t0.000000\t0\n",
      "middle\tage\t0.000000\t0.000000\t0\n",
      "modern\tbourgeois\t0.000000\t0.000000\t0\n",
      "private\tproperty\t0.000000\t0.000000\t0\n",
      "feudal\tsociety\t0.000000\t0.000000\t0\n",
      "middle\tclass\t0.000000\t0.000000\t0\n",
      "petty\tbourgeois\t0.000000\t0.000000\t0\n",
      "absolute\tmonarchy\t0.000000\t0.000000\t0\n",
      "modern\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "free\ttrade\t0.000000\t0.000000\t0\n",
      "bourgeois\tproduction\t0.000000\t0.000000\t1\n",
      "political\tbourgeoisie\t0.000000\t0.000000\t0\n",
      "immense\tmajority\t0.000000\t0.000000\t0\n",
      "french\trevolution\t0.000000\t0.000000\t0\n",
      "mere\tproduction\t0.000000\t0.000000\t0\n",
      "political\tsupremacy\t0.000000\t0.000000\t0\n",
      "eighteenth\tcentury\t0.000000\t0.000000\t0\n",
      "historical\tdevelopment\t0.000000\t0.000000\t0\n",
      "eternal\ttruth\t0.000000\t0.000000\t0\n",
      "undeveloped\tstate\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'J', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\tsocialism\t0.000000\t3.785939\t1\n",
      "communist\tparty\t0.000000\t2.863564\t0\n",
      "communist\tcommunist\t0.000000\t2.645751\t0\n",
      "england\tfrance\t0.000000\t2.516611\t0\n",
      "socialist\tliterature\t0.000000\t2.500000\t0\n",
      "communist\tliterature\t0.000000\t2.500000\t0\n",
      "communism\tpower\t0.000000\t1.527525\t0\n",
      "germany\tbourgeoisie\t0.000000\t0.577350\t0\n",
      "socialist\tcommunist\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'NNP', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implememnt a better lemmatizer for handling proper nouns (NNP / NNPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(word, pos):\n",
    "    if not pos.startswith('NNP'):\n",
    "        word = word.lower()\n",
    "    mapping = {'J': 'a', 'R': 'r', 'N': 'n', 'V': 'v'}\n",
    "    if pos[0] in mapping:\n",
    "        return wordnet_lemmatizer.lemmatize(word, mapping[pos[0]])\n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And do not lower() the word in the main function anymore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distant_collocations(tokens_with_tag, pos1, pos2, min_cut=2, window_size=9):\n",
    "    word_pair_counts = Counter()\n",
    "    word_pair_distance_counts = Counter()\n",
    "    for i in range(len(tokens_with_tag) - 1):\n",
    "        w1, t1 = tokens_with_tag[i]\n",
    "        if not t1.startswith(pos1):\n",
    "            continue\n",
    "        w1 = lemmatize(w1, t1)\n",
    "        for distance in range(1, window_size):\n",
    "            if i + distance < len(tokens_with_tag):\n",
    "                w2, t2 = tokens_with_tag[i + distance]\n",
    "                if t2.startswith(pos2):\n",
    "                    w2 = lemmatize(w2, t2)\n",
    "                    word_pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                    word_pair_counts[(w1, w2)] += 1\n",
    "    \n",
    "    pair_mean_distances = Counter()\n",
    "\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if word_pair_counts[(w1, w2)] > 1:\n",
    "            pair_mean_distances[(w1, w2)] += distance * (c / word_pair_counts[(w1, w2)])\n",
    "\n",
    "    pair_deviations = Counter()\n",
    "    for (w1, w2, distance), c in word_pair_distance_counts.most_common():\n",
    "        if w1 in stopword_list:\n",
    "            continue\n",
    "        if word_pair_counts[(w1, w2)] > min_cut:\n",
    "            pair_deviations[(w1, w2)] += c * ((distance - pair_mean_distances[(w1, w2)]) ** 2)\n",
    "    \n",
    "    for (w1, w2), dev_tmp in pair_deviations.most_common():\n",
    "        s_2 = dev_tmp / (word_pair_counts[(w1, w2)] - 1)\n",
    "        pair_deviations[(w1, w2)] = s_2 ** 0.5\n",
    "    \n",
    "    return pair_deviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Communist\tliterature\t0.000000\t2.886751\t0\n",
      "Socialist\tliterature\t0.000000\t2.886751\t0\n",
      "England\tFrance\t0.000000\t2.516611\t0\n",
      "Communism\tpower\t0.000000\t1.527525\t0\n",
      "Communists\tparty\t0.000000\t1.154701\t0\n",
      "Germany\tbourgeoisie\t0.000000\t0.577350\t0\n",
      "Socialist\tCommunist\t0.000000\t0.000000\t0\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'NNP', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find\twork\t2.666667\t2.886751\t3\n",
      "abolish\tproperty\t5.000000\t2.645751\t3\n",
      "join\tclass\t5.666667\t2.516611\t3\n",
      "fight\tbourgeoisie\t3.800000\t2.387467\t5\n",
      "work\tclass\t2.000000\t2.374103\t12\n",
      "represent\tinterest\t3.666667\t2.081666\t3\n",
      "mean\tbourgeois\t4.333333\t2.081666\t3\n",
      "exist\tthing\t4.000000\t1.732051\t3\n",
      "revolutionize\tproduction\t3.333333\t1.154701\t3\n",
      "intend\tproperty\t6.666667\t1.154701\t3\n",
      "concentrate\thand\t3.666667\t1.154701\t3\n",
      "attain\tend\t3.000000\t1.000000\t3\n",
      "exist\tsociety\t1.333333\t0.816497\t6\n",
      "produce\tproduct\t3.500000\t0.577350\t4\n",
      "appropriate\tproduct\t1.600000\t0.547723\t5\n",
      "rule\tclass\t1.000000\t0.000000\t4\n",
      "work\tparty\t2.000000\t0.000000\t4\n",
      "pave\tway\t2.000000\t0.000000\t3\n",
      "put\tend\t2.000000\t0.000000\t3\n",
      "lose\tcharacter\t3.000000\t0.000000\t3\n"
     ]
    }
   ],
   "source": [
    "collocations = distant_collocations(tokens_with_tag, 'V', 'N')\n",
    "\n",
    "for (w1, w2), dev in collocations.most_common()[-20:]:\n",
    "    print(\"%s\\t%s\\t%f\\t%f\\t%d\" % (w1, w2, pair_mean_distances[(w1, w2)], dev, word_pair_counts[(w1, w2)]))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
